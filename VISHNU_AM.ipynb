{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fe9df13",
   "metadata": {},
   "source": [
    "# To use colab python3 GPU that will more fast than local cpu\n",
    "# !/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f8ea6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/you/Downloads/files/appointment_system/claud-appoimnet/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent, DeepFool\n",
    "from art.estimators.classification import PyTorchClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c90b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "def load_cifar10_data(batch_size=128, num_samples=1000):\n",
    "    print(\"Loading CIFAR10 dataset...\")\n",
    "\n",
    "    # CIFAR10 normalization values\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "    # Load training and test datasets\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    # Use a subset for faster evaluation\n",
    "    if num_samples < len(test_dataset):\n",
    "        indices = np.random.choice(len(test_dataset), num_samples, replace=False)\n",
    "        test_dataset = Subset(test_dataset, indices)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Class names\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "    return train_loader, test_loader, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b08ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(num_classes=10):\n",
    "\n",
    "    print(\"Loading pre-trained ResNet-18 model...\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = torchvision.models.resnet18(pretrained=True) # Load pre-trained ResNet18 and modify for CIFAR10\n",
    "\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False) # conv layer for CIFAR10 (32x32 images)\n",
    "    model.maxpool = nn.Identity()  # Remove max pooling\n",
    "\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)  # Modify final layer for CIFAR10 (10 classes)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "690b2fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d71d9450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_art_classifier(model, device):\n",
    "    criterion = nn.CrossEntropyLoss()     # Define loss function and optimizer (not used for inference but required by ART)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    classifier = PyTorchClassifier(\n",
    "        model=model,\n",
    "        loss=criterion,\n",
    "        optimizer=optimizer,\n",
    "        input_shape=(3, 32, 32),\n",
    "        nb_classes=10,\n",
    "        clip_values=(0.0, 1.0),\n",
    "        device_type='gpu' if device.type == 'cuda' else 'cpu'\n",
    "    )\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "563e8421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_art_classifier(model, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    classifier = PyTorchClassifier(\n",
    "        model=model,\n",
    "        loss=criterion,\n",
    "        optimizer=optimizer,\n",
    "        input_shape=(3, 32, 32),\n",
    "        nb_classes=10,\n",
    "        clip_values=(0.0, 1.0),\n",
    "        device_type='gpu' if device.type == 'cuda' else 'cpu'\n",
    "    )\n",
    "\n",
    "    return classifier\n",
    "\n",
    "\n",
    "def generate_adversarial_examples(classifier, attack_name, x_test, y_test, epsilon=0.03):\n",
    "    print(f\"\\nGenerating adversarial examples using {attack_name.upper()}...\")\n",
    "    print(f\"Epsilon: {epsilon}\")\n",
    "\n",
    "    if attack_name.lower() == 'fgsm':\n",
    "        attack = FastGradientMethod(estimator=classifier, eps=epsilon)\n",
    "    elif attack_name.lower() == 'pgd':\n",
    "        attack = ProjectedGradientDescent(\n",
    "            estimator=classifier,\n",
    "            eps=epsilon,\n",
    "            eps_step=epsilon/10,\n",
    "            max_iter=40,\n",
    "            targeted=False\n",
    "        )\n",
    "    elif attack_name.lower() == 'deepfool':\n",
    "        attack = DeepFool(classifier=classifier, max_iter=50, epsilon=1e-6)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown attack: {attack_name}\")\n",
    "\n",
    "    # Generate adversarial examples\n",
    "    x_adv = attack.generate(x=x_test)\n",
    "\n",
    "    return x_adv, attack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_adversarial_robustness(classifier, x_test, y_test, attack_configs):\n",
    "    print(\"\\nEvaluating on clean data...\")\n",
    "    clean_preds = classifier.predict(x_test)\n",
    "    clean_accuracy = np.mean(np.argmax(clean_preds, axis=1) == np.argmax(y_test, axis=1)) * 100\n",
    "    print(f\"Clean Accuracy: {clean_accuracy:.2f}%\")\n",
    "\n",
    "    results = {\n",
    "        'clean': {\n",
    "            'accuracy': clean_accuracy,\n",
    "            'predictions': clean_preds\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for attack_config in attack_configs:\n",
    "        attack_name = attack_config['name']\n",
    "        epsilon = attack_config.get('epsilon', 0.03)\n",
    "\n",
    "        try:\n",
    "            x_adv, attack = generate_adversarial_examples(\n",
    "                classifier, attack_name, x_test, y_test, epsilon\n",
    "            )\n",
    "\n",
    "            print(f\"Evaluating on {attack_name.upper()} adversarial examples...\")\n",
    "            adv_preds = classifier.predict(x_adv)\n",
    "            adv_accuracy = np.mean(np.argmax(adv_preds, axis=1) == np.argmax(y_test, axis=1)) * 100\n",
    "            print(f\"{attack_name.upper()} Adversarial Accuracy: {adv_accuracy:.2f}%\")\n",
    "\n",
    "            perturbation = np.abs(x_adv - x_test)\n",
    "            avg_perturbation = np.mean(perturbation)\n",
    "            max_perturbation = np.max(perturbation)\n",
    "\n",
    "            results[attack_name] = {\n",
    "                'accuracy': adv_accuracy,\n",
    "                'predictions': adv_preds,\n",
    "                'adversarial_examples': x_adv,\n",
    "                'avg_perturbation': avg_perturbation,\n",
    "                'max_perturbation': max_perturbation,\n",
    "                'epsilon': epsilon\n",
    "            }\n",
    "\n",
    "            print(f\"Average perturbation: {avg_perturbation:.6f}\")\n",
    "            print(f\"Max perturbation: {max_perturbation:.6f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nWarning: {attack_name.upper()} attack failed: {str(e)}\")\n",
    "            print(f\"Skipping {attack_name.upper()} attack and continuing with others...\")\n",
    "            continue\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def visualize_accuracy_comparison(results, output_path='results/accuracy_comparison.png'):\n",
    "    print(\"\\nCreating accuracy comparison plot...\")\n",
    "    attack_names = list(results.keys())\n",
    "    accuracies = [results[name]['accuracy'] for name in attack_names]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(range(len(attack_names)), accuracies, color=['green', 'red', 'orange', 'purple'][:len(attack_names)])\n",
    "\n",
    "    plt.xlabel('Attack Type', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "    plt.title('Model Accuracy: Clean vs Adversarial Examples', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(range(len(attack_names)), [name.upper() for name in attack_names])\n",
    "    plt.ylim(0, 100)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved accuracy comparison to {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def visualize_adversarial_examples(x_clean, x_adv_dict, y_true, classes,\n",
    "                                   num_samples=5, output_path='results/adversarial_examples.png'):\n",
    "    print(\"\\nCreating adversarial examples visualization...\")\n",
    "\n",
    "    # Denormalize for visualization\n",
    "    mean = np.array([0.4914, 0.4822, 0.4465]).reshape(1, 3, 1, 1)\n",
    "    std = np.array([0.2023, 0.1994, 0.2010]).reshape(1, 3, 1, 1)\n",
    "\n",
    "    def denormalize(x):\n",
    "        x_denorm = x * std + mean\n",
    "        return np.clip(x_denorm, 0, 1)\n",
    "\n",
    "    # Select random samples\n",
    "    indices = np.random.choice(len(x_clean), num_samples, replace=False)\n",
    "\n",
    "    # Get attack names (excluding 'clean')\n",
    "    attack_names = [name for name in x_adv_dict.keys() if name != 'clean']\n",
    "\n",
    "    # Create subplots\n",
    "    num_cols = len(attack_names) + 1  # +1 for original\n",
    "    fig, axes = plt.subplots(num_samples, num_cols, figsize=(4*num_cols, 4*num_samples))\n",
    "\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        # Original image\n",
    "        img_clean = denormalize(x_clean[idx:idx+1])[0].transpose(1, 2, 0)\n",
    "        true_label = np.argmax(y_true[idx])\n",
    "\n",
    "        axes[i, 0].imshow(img_clean)\n",
    "        axes[i, 0].set_title(f'Original\\nTrue: {classes[true_label]}', fontsize=10, fontweight='bold')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # Adversarial images\n",
    "        for j, attack_name in enumerate(attack_names):\n",
    "            x_adv = x_adv_dict[attack_name]\n",
    "            img_adv = denormalize(x_adv[idx:idx+1])[0].transpose(1, 2, 0)\n",
    "\n",
    "            # Calculate perturbation\n",
    "            perturbation = np.abs(img_adv - img_clean)\n",
    "            avg_pert = np.mean(perturbation)\n",
    "\n",
    "            axes[i, j+1].imshow(img_adv)\n",
    "            axes[i, j+1].set_title(f'{attack_name.upper()}\\nPert: {avg_pert:.4f}',\n",
    "                                   fontsize=10, fontweight='bold')\n",
    "            axes[i, j+1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved adversarial examples to {output_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5269be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_perturbations(x_clean, x_adv_dict, num_samples=5, output_path='results/perturbation_magnitudes.png'):\n",
    "\n",
    "    print(\"\\nCreating perturbation visualization...\")\n",
    "\n",
    "    indices = np.random.choice(len(x_clean), num_samples, replace=False)\n",
    "\n",
    "    attack_names = [name for name in x_adv_dict.keys() if name != 'clean']\n",
    "\n",
    "    fig, axes = plt.subplots(num_samples, len(attack_names), figsize=(4*len(attack_names), 4*num_samples))\n",
    "\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    if len(attack_names) == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        for j, attack_name in enumerate(attack_names):\n",
    "            x_adv = x_adv_dict[attack_name]\n",
    "\n",
    "            perturbation = np.abs(x_adv[idx] - x_clean[idx])\n",
    "            perturbation = perturbation.transpose(1, 2, 0)\n",
    "            perturbation = np.mean(perturbation, axis=2)  # Average across channels\n",
    "\n",
    "            im = axes[i, j].imshow(perturbation, cmap='hot')\n",
    "            axes[i, j].set_title(f'{attack_name.upper()}\\nMax: {np.max(perturbation):.4f}',\n",
    "                               fontsize=10, fontweight='bold')\n",
    "            axes[i, j].axis('off')\n",
    "            plt.colorbar(im, ax=axes[i, j], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved perturbation magnitudes to {output_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14b9cc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_analysis_report(results, output_path='results/analysis_report.txt'):\n",
    "\n",
    "    print(\"\\nGenerating analysis report...\")\n",
    "\n",
    "    report = []\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"ADVERSARIAL ROBUSTNESS EVALUATION REPORT\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Clean accuracy\n",
    "    clean_acc = results['clean']['accuracy']\n",
    "    report.append(f\"Clean Data Accuracy: {clean_acc:.2f}%\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Attack results\n",
    "    report.append(\"Adversarial Attack Results:\")\n",
    "    report.append(\"-\" * 80)\n",
    "\n",
    "    for attack_name, data in results.items():\n",
    "        if attack_name == 'clean':\n",
    "            continue\n",
    "\n",
    "        acc = data['accuracy']\n",
    "        epsilon = data.get('epsilon', 'N/A')\n",
    "        avg_pert = data.get('avg_perturbation', 0)\n",
    "        max_pert = data.get('max_perturbation', 0)\n",
    "\n",
    "        accuracy_drop = clean_acc - acc\n",
    "\n",
    "        report.append(f\"\\n{attack_name.upper()} Attack:\")\n",
    "        report.append(f\"  - Epsilon: {epsilon}\")\n",
    "        report.append(f\"  - Adversarial Accuracy: {acc:.2f}%\")\n",
    "        report.append(f\"  - Accuracy Drop: {accuracy_drop:.2f}%\")\n",
    "        report.append(f\"  - Average Perturbation: {avg_pert:.6f}\")\n",
    "        report.append(f\"  - Max Perturbation: {max_pert:.6f}\")\n",
    "\n",
    "    report.append(\"\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"ANALYSIS AND INSIGHTS\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Analysis\n",
    "    report.append(\"1. ATTACK EFFECTIVENESS:\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Find most and least effective attacks\n",
    "    attack_accs = {name: data['accuracy'] for name, data in results.items() if name != 'clean'}\n",
    "    most_effective = min(attack_accs, key=attack_accs.get)\n",
    "    least_effective = max(attack_accs, key=attack_accs.get)\n",
    "\n",
    "    report.append(f\"   - Most effective attack: {most_effective.upper()} \"\n",
    "                 f\"(accuracy dropped to {attack_accs[most_effective]:.2f}%)\")\n",
    "    report.append(f\"   - Least effective attack: {least_effective.upper()} \"\n",
    "                 f\"(accuracy dropped to {attack_accs[least_effective]:.2f}%)\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    report.append(\"2. CONCEPTUAL DIFFERENCES BETWEEN ATTACKS:\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"   FGSM (Fast Gradient Sign Method):\")\n",
    "    report.append(\"   - Single-step attack using the sign of the gradient\")\n",
    "    report.append(\"   - Fast to compute but less sophisticated\")\n",
    "    report.append(\"   - Perturbation: x_adv = x + ε * sign(∇_x L(θ, x, y))\")\n",
    "    report.append(\"   - Pros: Very fast, good for adversarial training\")\n",
    "    report.append(\"   - Cons: Less effective than iterative methods\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    report.append(\"   PGD (Projected Gradient Descent):\")\n",
    "    report.append(\"   - Iterative attack with multiple small steps\")\n",
    "    report.append(\"   - Projects perturbation back to epsilon ball after each step\")\n",
    "    report.append(\"   - Considered one of the strongest first-order attacks\")\n",
    "    report.append(\"   - Pros: More effective, better explores adversarial space\")\n",
    "    report.append(\"   - Cons: Computationally expensive (multiple iterations)\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    if 'deepfool' in results:\n",
    "        report.append(\"   DeepFool:\")\n",
    "        report.append(\"   - Finds minimal perturbation to cross decision boundary\")\n",
    "        report.append(\"   - Uses geometric approach to find optimal direction\")\n",
    "        report.append(\"   - Iteratively linearizes the classifier\")\n",
    "        report.append(\"   - Pros: Minimal perturbations, geometrically meaningful\")\n",
    "        report.append(\"   - Cons: More complex, computationally intensive\")\n",
    "        report.append(\"\")\n",
    "\n",
    "    report.append(\"3. OBSERVED VULNERABILITIES:\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Calculate average accuracy drop\n",
    "    avg_drop = np.mean([clean_acc - data['accuracy'] for name, data in results.items() if name != 'clean'])\n",
    "\n",
    "    if avg_drop > 40:\n",
    "        report.append(f\"   - The model shows SIGNIFICANT vulnerability to adversarial attacks\")\n",
    "        report.append(f\"   - Average accuracy drop: {avg_drop:.2f}%\")\n",
    "        report.append(\"   - This indicates the model relies heavily on non-robust features\")\n",
    "    elif avg_drop > 20:\n",
    "        report.append(f\"   - The model shows MODERATE vulnerability to adversarial attacks\")\n",
    "        report.append(f\"   - Average accuracy drop: {avg_drop:.2f}%\")\n",
    "        report.append(\"   - There is substantial room for robustness improvement\")\n",
    "    else:\n",
    "        report.append(f\"   - The model shows RELATIVELY LOW vulnerability to adversarial attacks\")\n",
    "        report.append(f\"   - Average accuracy drop: {avg_drop:.2f}%\")\n",
    "        report.append(\"   - The model exhibits some inherent robustness\")\n",
    "\n",
    "    report.append(\"\")\n",
    "    report.append(\"4. MITIGATION STRATEGIES:\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"   Recommended approaches to improve adversarial robustness:\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"   a) Adversarial Training:\")\n",
    "    report.append(\"      - Train on mix of clean and adversarial examples\")\n",
    "    report.append(\"      - Use PGD-generated adversarial examples during training\")\n",
    "    report.append(\"      - Most effective defense but computationally expensive\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"   b) Input Preprocessing:\")\n",
    "    report.append(\"      - Apply transformations (JPEG compression, bit-depth reduction)\")\n",
    "    report.append(\"      - Use denoising autoencoders\")\n",
    "    report.append(\"      - Random resizing and padding\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"   c) Certified Defenses:\")\n",
    "    report.append(\"      - Randomized smoothing\")\n",
    "    report.append(\"      - Provable robustness guarantees within epsilon ball\")\n",
    "    report.append(\"      - Trade-off between robustness and accuracy\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"   d) Ensemble Methods:\")\n",
    "    report.append(\"      - Use multiple models with different architectures\")\n",
    "    report.append(\"      - Adversarial examples may not transfer well\")\n",
    "    report.append(\"      - Increases computational cost\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"   e) Detection Methods:\")\n",
    "    report.append(\"      - Train adversarial example detectors\")\n",
    "    report.append(\"      - Reject suspicious inputs\")\n",
    "    report.append(\"      - Can be bypassed by adaptive attacks\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    report.append(\"5. RECOMMENDATIONS:\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"   Based on the evaluation results:\")\n",
    "    report.append(f\"   - Priority: Implement adversarial training with PGD (ε={results.get('pgd', {}).get('epsilon', 0.03)})\")\n",
    "    report.append(\"   - Consider: Input preprocessing as additional defense layer\")\n",
    "    report.append(\"   - Evaluate: Trade-off between clean and robust accuracy\")\n",
    "    report.append(\"   - Monitor: Model performance on diverse adversarial attacks\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"END OF REPORT\")\n",
    "    report.append(\"=\" * 80)\n",
    "\n",
    "    # Save report\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write('\\n'.join(report))\n",
    "\n",
    "    print(f\"Saved analysis report to {output_path}\")\n",
    "\n",
    "    # Also print to console\n",
    "    print(\"\\n\" + \"\\n\".join(report))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f89df3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ADVERSARIAL ROBUSTNESS EVALUATION PIPELINE\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "    NUM_TEST_SAMPLES = 1000  # Use subset for faster evaluation\n",
    "    BATCH_SIZE = 128\n",
    "    EPSILON = 0.03  # Perturbation magnitude\n",
    "\n",
    "    train_loader, test_loader, classes = load_cifar10_data( batch_size=BATCH_SIZE,num_samples=NUM_TEST_SAMPLES)\n",
    "\n",
    "    # Step 2: Load pre-trained model\n",
    "    model, device = load_pretrained_model(num_classes=10)\n",
    "\n",
    "    # Step 3: Evaluate clean accuracy\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATING CLEAN ACCURACY\")\n",
    "    print(\"=\"*80)\n",
    "    clean_accuracy = evaluate_model(model, test_loader, device)\n",
    "    print(f\"\\nClean Test Accuracy: {clean_accuracy:.2f}%\")\n",
    "\n",
    "    # Step 4: Prepare data for ART\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PREPARING DATA FOR ADVERSARIAL ATTACKS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Get test data as numpy arrays\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        x_test_list.append(images.numpy())\n",
    "        y_test_list.append(labels.numpy())\n",
    "\n",
    "    x_test = np.concatenate(x_test_list, axis=0)\n",
    "    y_test_labels = np.concatenate(y_test_list, axis=0)\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    y_test = np.eye(10)[y_test_labels]\n",
    "\n",
    "    print(f\"Test data shape: {x_test.shape}\")\n",
    "    print(f\"Test labels shape: {y_test.shape}\")\n",
    "\n",
    "    # Step 5: Create ART classifier\n",
    "    classifier = create_art_classifier(model, device)\n",
    "\n",
    "    # Step 6: Generate and evaluate adversarial examples\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING AND EVALUATING ADVERSARIAL EXAMPLES\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    attack_configs = [\n",
    "        {'name': 'fgsm', 'epsilon': EPSILON},\n",
    "        {'name': 'pgd', 'epsilon': EPSILON},]\n",
    "\n",
    "    results = evaluate_adversarial_robustness(classifier, x_test, y_test, attack_configs)\n",
    "\n",
    "    # Step 7: Create visualizations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CREATING VISUALIZATIONS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Accuracy comparison\n",
    "    visualize_accuracy_comparison(results)\n",
    "\n",
    "    # Adversarial examples\n",
    "    x_adv_dict = {name: data.get('adversarial_examples', x_test)\n",
    "                  for name, data in results.items()}\n",
    "    visualize_adversarial_examples(x_test, x_adv_dict, y_test, classes)\n",
    "\n",
    "    # Perturbation magnitudes\n",
    "    visualize_perturbations(x_test, x_adv_dict)\n",
    "\n",
    "    # Step 8: Generate analysis report\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING ANALYSIS REPORT\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    generate_analysis_report(results)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nResults saved in 'results/' directory:\")\n",
    "    print(\"  - accuracy_comparison.png\")\n",
    "    print(\"  - adversarial_examples.png\")\n",
    "    print(\"  - perturbation_magnitudes.png\")\n",
    "    print(\"  - analysis_report.txt\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
