# Pipeline vs Training: Comparison Guide

## Table of Contents

1. [Overview](#overview)
2. [Quick Comparison](#quick-comparison)
3. [Adversarial Pipeline (Evaluation)](#adversarial-pipeline-evaluation)
4. [Adversarial Training (Mitigation)](#adversarial-training-mitigation)
5. [Key Differences](#key-differences)
6. [Results Comparison](#results-comparison)
7. [When to Use Each](#when-to-use-each)
8. [Workflow Recommendations](#workflow-recommendations)

---

## Overview

This project includes **two distinct approaches** to adversarial robustness:

### 1. **Adversarial Pipeline** (`adversarial_pipeline.py`)
**Purpose**: **Evaluate** the robustness of an existing pre-trained model

**What it does**:
- Takes a pre-trained model (already trained on clean data)
- Tests it against adversarial attacks
- Measures how vulnerable it is
- **Does NOT modify or improve the model**

**Analogy**: Like a security audit - you're testing existing defenses

### 2. **Adversarial Training** (`adversarial_training.py`)
**Purpose**: **Improve** model robustness by training with adversarial examples

**What it does**:
- Trains a model from scratch (or fine-tunes)
- Includes adversarial examples during training
- Creates a more robust model
- **Modifies the model to be stronger**

**Analogy**: Like hardening defenses - you're building better protection

---

## Quick Comparison

| Aspect | Adversarial Pipeline | Adversarial Training |
|--------|---------------------|----------------------|
| **Purpose** | Evaluation & Testing | Defense & Mitigation |
| **Model State** | Pre-trained (frozen) | Training (updating weights) |
| **Computational Cost** | Low (~5-15 min) | High (~30-90 min) |
| **GPU Requirement** | Optional | Strongly recommended |
| **Output** | Vulnerability metrics | Robust model + metrics |
| **Model Changes** | âŒ No changes | âœ… Model improved |
| **Primary Goal** | "How vulnerable is my model?" | "How can I make it robust?" |
| **Use Case** | Assessment, Research | Production, Deployment |

---

## Adversarial Pipeline (Evaluation)

### What It Is

A **diagnostic tool** that evaluates existing model robustness without any training.

### Process Flow

```
Pre-trained Model (ResNet-18)
         â†“
   Load Model (frozen)
         â†“
   Generate Adversarial Examples
   â”œâ”€â”€ FGSM Attack
   â””â”€â”€ PGD Attack
         â†“
   Evaluate Accuracy
   â”œâ”€â”€ Clean Data: ~85-90%
   â”œâ”€â”€ FGSM Attack: ~40-50%
   â””â”€â”€ PGD Attack: ~10-25%
         â†“
   Generate Visualizations & Reports
```

### Key Characteristics

**âœ… Advantages**:
- **Fast**: Completes in 5-15 minutes
- **No training needed**: Works with any pre-trained model
- **Low resource**: Runs on CPU (GPU optional)
- **Comprehensive analysis**: Detailed vulnerability assessment

**âŒ Limitations**:
- **Read-only**: Doesn't improve the model
- **Static**: Model remains vulnerable after evaluation
- **No defense**: Only identifies problems, doesn't fix them

### Expected Results

#### Clean Accuracy
```
ResNet-18 on CIFAR-10: ~85-90%
(Pre-trained model performance on normal images)
```

#### Adversarial Accuracy
```
FGSM (Îµ=0.03):  ~40-50%  (40-50% accuracy drop)
PGD (Îµ=0.03):   ~10-25%  (60-75% accuracy drop)
```

**Interpretation**:
- Large accuracy drop = Model is vulnerable
- FGSM is weaker, PGD is stronger
- Results show the model is NOT robust by default

### Outputs Generated

```
results/
â”œâ”€â”€ accuracy_comparison.png          # Bar chart: Clean vs Adversarial
â”œâ”€â”€ adversarial_examples.png         # Visual samples of attacks
â”œâ”€â”€ perturbation_magnitudes.png      # Heatmaps of perturbations
â””â”€â”€ analysis_report.txt              # Detailed vulnerability analysis
```

### Code Snippet

```python
# Load pre-trained model
model = load_pretrained_model()  # Frozen, no training

# Generate attacks
x_adv_fgsm = fgsm_attack.generate(x_clean)
x_adv_pgd = pgd_attack.generate(x_clean)

# Evaluate (model unchanged)
clean_acc = evaluate(model, x_clean)      # ~85%
fgsm_acc = evaluate(model, x_adv_fgsm)    # ~45%
pgd_acc = evaluate(model, x_adv_pgd)      # ~15%
```

---

## Adversarial Training (Mitigation)

### What It Is

A **defense mechanism** that trains models to be robust by including adversarial examples during training.

### Process Flow

```
Standard Model Training          Adversarial Training
(Clean Data Only)         vs     (Clean + Adversarial Data)
         â†“                                 â†“
   Train on Clean              Train on Clean + Adversarial
         â†“                                 â†“
   Clean Acc: ~85%                  Clean Acc: ~75-80%
   FGSM Acc: ~40%                   FGSM Acc: ~60-70%
   PGD Acc: ~10%                    PGD Acc: ~40-55%
         â†“                                 â†“
   Vulnerable Model                 Robust Model
```

### Key Characteristics

**âœ… Advantages**:
- **Creates robustness**: Model learns to resist attacks
- **Generalizes**: Often robust to attacks it wasn't trained on
- **Production-ready**: Can deploy robust models
- **Transferable**: Works across different attack types

**âŒ Limitations**:
- **Slow**: Takes 30-90 minutes (or hours for full training)
- **Resource-intensive**: Requires good GPU
- **Clean accuracy trade-off**: May sacrifice 5-15% clean accuracy
- **Expensive**: ~40Ã— slower than standard training

### Training Algorithm

```python
for epoch in epochs:
    for batch in training_data:
        # Get clean images
        x_clean, y_true = batch

        # Generate adversarial examples
        model.eval()
        with torch.no_grad():
            x_adv = pgd_attack.generate(x_clean)
        model.train()

        # Train on BOTH clean and adversarial
        loss_clean = loss_fn(model(x_clean), y_true)
        loss_adv = loss_fn(model(x_adv), y_true)

        # Combined loss (Î± = balance parameter)
        loss = (1-Î±) * loss_clean + Î± * loss_adv

        # Update model weights
        loss.backward()
        optimizer.step()
```

### Expected Results

#### Standard Training (Baseline)
```
Clean Accuracy:     ~85%
FGSM Accuracy:      ~40%  (45% drop)
PGD Accuracy:       ~10%  (75% drop)

Conclusion: Vulnerable to attacks
```

#### Adversarial Training (Î±=0.5)
```
Clean Accuracy:     ~75-80%  (5-10% drop from baseline)
FGSM Accuracy:      ~60-70%  (20-30% improvement)
PGD Accuracy:       ~40-55%  (30-45% improvement)

Conclusion: Much more robust, acceptable clean accuracy trade-off
```

### Performance Comparison Chart

```
Accuracy (%)
100 â”¤
 90 â”¤ â–ˆâ–ˆâ–ˆâ–ˆ Clean (Standard)
 80 â”¤ â–ˆâ–ˆâ–ˆâ–ˆ Clean (Adversarial Training)
 70 â”¤      â–ˆâ–ˆâ–ˆâ–ˆ FGSM (Adversarial Training)
 60 â”¤
 50 â”¤
 40 â”¤ â–ˆâ–ˆâ–ˆâ–ˆ FGSM (Standard)
 30 â”¤      â–ˆâ–ˆâ–ˆâ–ˆ PGD (Adversarial Training)
 20 â”¤
 10 â”¤ â–ˆâ–ˆâ–ˆâ–ˆ PGD (Standard)
  0 â”¤
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### Outputs Generated

```
results/
â”œâ”€â”€ mitigation_comparison.png        # Standard vs Adversarial Training
â”œâ”€â”€ adversarial_training_history.png # Training curves (loss, accuracy)
â””â”€â”€ mitigation_report.txt            # Robustness improvement analysis
```

### Code Snippet

```python
# Standard Training
model_standard = train(data, adversarial=False)
# Result: High clean acc, Low adversarial acc

# Adversarial Training
model_robust = train(data, adversarial=True, alpha=0.5)
# Result: Moderate clean acc, High adversarial acc

# Comparison
print(f"Standard - Clean: {std_clean}%, PGD: {std_pgd}%")
print(f"Robust   - Clean: {rob_clean}%, PGD: {rob_pgd}%")
```

---

## Key Differences

### 1. **Model State**

| Aspect | Pipeline | Training |
|--------|----------|----------|
| Model weights | **Frozen** (no updates) | **Updated** (learned) |
| Gradient computation | Only for attacks | For attacks + training |
| Model improvement | âŒ No | âœ… Yes |

### 2. **Computational Requirements**

| Resource | Pipeline | Training |
|----------|----------|----------|
| Time | 5-15 minutes | 30-90 minutes |
| GPU memory | 2-4 GB | 4-8 GB |
| CPU option | âœ… Viable | âš ï¸ Very slow |
| Parallelization | Batch processing | Batch + distributed |

### 3. **Purpose & Output**

| Purpose | Pipeline | Training |
|---------|----------|----------|
| Primary goal | Measure vulnerability | Build robustness |
| Output type | Metrics & visualizations | Model + metrics |
| Actionable result | "Model is vulnerable" | "Model is now robust" |
| Deployment ready | âŒ No | âœ… Yes |

### 4. **Attack Usage**

| Aspect | Pipeline | Training |
|--------|----------|----------|
| Attack purpose | **Test** model | **Train** model |
| Attack frequency | Once per evaluation | Every training batch |
| Attack parameters | Strong (Îµ=0.03, 40 iter) | Moderate (Îµ=0.03, 10 iter) |
| Computational cost | Low (single pass) | High (repeated) |

### 5. **Results Interpretation**

**Pipeline Results**:
```
Clean: 85%, FGSM: 45%, PGD: 15%

Interpretation:
â†’ Model is VULNERABLE
â†’ Needs defense mechanism
â†’ Not suitable for adversarial settings
```

**Training Results**:
```
Before: Clean: 85%, PGD: 15%
After:  Clean: 78%, PGD: 48%

Interpretation:
â†’ Robustness IMPROVED by 33%
â†’ Small clean accuracy cost (7%)
â†’ Suitable for adversarial settings
```

---

## Results Comparison

### Typical Performance Metrics

#### Pre-trained Model (Pipeline Evaluation)

| Attack | Accuracy | Drop from Clean | Status |
|--------|----------|-----------------|--------|
| Clean  | 85-90% | 0% | âœ… Good |
| FGSM   | 40-50% | 40-45% | âŒ Vulnerable |
| PGD    | 10-25% | 60-75% | âŒ Very Vulnerable |

**Verdict**: Model performs well on clean data but is **extremely vulnerable** to adversarial attacks.

#### Adversarially Trained Model

| Attack | Standard Model | Adversarial Model | Improvement |
|--------|---------------|-------------------|-------------|
| Clean  | 85% | 78% | -7% (trade-off) |
| FGSM   | 45% | 65% | **+20%** ğŸ¯ |
| PGD    | 15% | 45% | **+30%** ğŸ¯ |

**Verdict**: Small clean accuracy sacrifice (7%) for **significant robustness gains** (20-30%).

### Visual Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Standard Model vs Adversarial Training          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Clean Accuracy                                          â”‚
â”‚  Standard:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 85%                    â”‚
â”‚  Robust:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–“â–“â–“ 78%  (-7%)             â”‚
â”‚                                                          â”‚
â”‚  FGSM Robustness                                         â”‚
â”‚  Standard:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 45%                    â”‚
â”‚  Robust:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–“ 65%  (+20%)  â­        â”‚
â”‚                                                          â”‚
â”‚  PGD Robustness                                          â”‚
â”‚  Standard:  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 15%                    â”‚
â”‚  Robust:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ 45%  (+30%)  â­        â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Legend:
â–ˆ = Model accuracy
â–‘ = Accuracy loss
â–“ = Improvement area
â­ = Significant gain
```

### Trade-off Analysis

```
Robustness vs Clean Accuracy Trade-off
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Clean Accuracy Loss:     -7%     (Acceptable)
FGSM Robustness Gain:    +20%    (Significant)
PGD Robustness Gain:     +30%    (Excellent)

Trade-off Ratio: 1% clean â†’ 3-4% adversarial
Recommendation: âœ… Worth it for adversarial settings
```

---

## When to Use Each

### Use Adversarial Pipeline When:

âœ… **Evaluating existing models**
- You have a pre-trained model and want to test its robustness
- Quick security audit needed
- Research: comparing different architectures' vulnerabilities

âœ… **Initial assessment**
- First step: understand current vulnerability level
- Before deciding whether to invest in adversarial training
- Baseline measurements for comparison

âœ… **Limited resources**
- No GPU available or limited compute budget
- Need quick results (minutes, not hours)
- Exploring multiple models quickly

âœ… **Analysis and research**
- Studying attack transferability
- Comparing attack methods (FGSM vs PGD)
- Understanding adversarial examples visually

**Example Scenarios**:
```
Scenario 1: "Is my model safe to deploy in adversarial settings?"
â†’ Use Pipeline: Quick 10-minute evaluation shows vulnerability

Scenario 2: "Which architecture is more robust: ResNet vs VGG?"
â†’ Use Pipeline: Test both models without retraining

Scenario 3: "What does an adversarial example look like?"
â†’ Use Pipeline: Generate and visualize examples
```

### Use Adversarial Training When:

âœ… **Building production systems**
- Model will face real-world adversarial attacks
- Security is critical (e.g., malware detection, spam filtering)
- Adversarial robustness is a requirement

âœ… **Improving model robustness**
- Pipeline showed significant vulnerabilities
- Need to harden defenses
- Willing to trade some clean accuracy for robustness

âœ… **Research on defenses**
- Testing mitigation strategies
- Comparing defense effectiveness
- Publishing robustness benchmarks

âœ… **Long-term deployment**
- Model will be deployed for extended periods
- Updates are expensive or infrequent
- Robust-by-design approach needed

**Example Scenarios**:
```
Scenario 1: "Our spam filter is being fooled by adversarial emails"
â†’ Use Training: Retrain with adversarial examples to resist attacks

Scenario 2: "Medical image classifier must be robust to perturbations"
â†’ Use Training: Safety-critical application requires robustness

Scenario 3: "Pipeline showed 70% accuracy drop under attack"
â†’ Use Training: Improve model to reduce vulnerability
```

---

## Workflow Recommendations

### Recommended Development Flow

```
Step 1: Initial Evaluation (Pipeline)
â”œâ”€â”€ Run adversarial_pipeline.py
â”œâ”€â”€ Assess vulnerability level
â”œâ”€â”€ Identify weak points
â””â”€â”€ Decision point: Is model vulnerable?
         â”‚
         â”œâ”€ No (robust enough) â”€â”€â†’ Deploy
         â”‚
         â””â”€ Yes (vulnerable) â”€â”€â†’ Step 2

Step 2: Defense Implementation (Training)
â”œâ”€â”€ Run adversarial_training.py
â”œâ”€â”€ Train robust model
â”œâ”€â”€ Measure improvement
â””â”€â”€ Decision point: Good trade-off?
         â”‚
         â”œâ”€ Yes â”€â”€â†’ Step 3
         â”‚
         â””â”€ No â”€â”€â†’ Adjust parameters, repeat

Step 3: Final Evaluation (Pipeline)
â”œâ”€â”€ Test robust model with pipeline
â”œâ”€â”€ Compare before/after metrics
â”œâ”€â”€ Verify improvement
â””â”€â”€ Deploy robust model
```

### Practical Example

**Project**: Deploy CIFAR-10 classifier in adversarial environment

```
Week 1: Assessment
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Run adversarial_pipeline.py          â”‚
â”‚                                       â”‚
â”‚ Results:                              â”‚
â”‚ - Clean: 87%                         â”‚
â”‚ - FGSM: 42%  â† Vulnerable!          â”‚
â”‚ - PGD: 13%   â† Very vulnerable!     â”‚
â”‚                                       â”‚
â”‚ Decision: Need adversarial training  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Week 2: Defense
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Run adversarial_training.py          â”‚
â”‚                                       â”‚
â”‚ Training for 10 epochs (~2 hours)    â”‚
â”‚                                       â”‚
â”‚ Results:                              â”‚
â”‚ - Clean: 79% (-8%)                   â”‚
â”‚ - FGSM: 67% (+25%)  â† Improved!     â”‚
â”‚ - PGD: 48% (+35%)   â† Much better!  â”‚
â”‚                                       â”‚
â”‚ Decision: Good trade-off, deploy     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Week 3: Verification
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Re-run adversarial_pipeline.py       â”‚
â”‚ on robust model                       â”‚
â”‚                                       â”‚
â”‚ Confirm improvements                  â”‚
â”‚ Generate deployment report            â”‚
â”‚ Ready for production                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Parameter Tuning Guide

#### For Pipeline (Evaluation)

```python
# Faster evaluation (less precise)
NUM_TEST_SAMPLES = 500
EPSILON = 0.03
max_iter = 20  # PGD iterations

# More thorough evaluation (slower)
NUM_TEST_SAMPLES = 5000
EPSILON = 0.03
max_iter = 100
```

#### For Training (Defense)

```python
# Quick training (lower robustness)
epochs = 5
alpha = 0.3  # Less adversarial examples
max_iter = 7  # Weaker attacks

# Strong training (better robustness)
epochs = 20
alpha = 0.7  # More adversarial examples
max_iter = 10  # Stronger attacks

# Balanced (recommended)
epochs = 10
alpha = 0.5  # Equal mix
max_iter = 10  # Moderate attacks
```

---

## Summary Table

| Question | Pipeline | Training |
|----------|----------|----------|
| **What does it do?** | Tests vulnerability | Builds robustness |
| **Changes model?** | No | Yes |
| **Runtime** | 5-15 min | 30-90 min |
| **Primary output** | Metrics | Robust model |
| **When to use** | Assessment | Defense |
| **GPU needed** | Optional | Recommended |
| **Clean accuracy** | Unchanged | May decrease 5-10% |
| **Adversarial accuracy** | Measured | Improved 20-40% |
| **Cost** | Low | High |
| **Deployment** | Not ready | Ready |

---

## Key Takeaways

### Adversarial Pipeline (Evaluation)
```
ğŸ¯ Purpose: Diagnose vulnerabilities
âš¡ Speed: Fast (minutes)
ğŸ“Š Output: Metrics & visualizations
ğŸ”’ Security: Identifies problems
ğŸ’° Cost: Low computational cost
```

### Adversarial Training (Mitigation)
```
ğŸ¯ Purpose: Build robustness
â±ï¸ Speed: Slow (hours)
ğŸ“¦ Output: Robust model
ğŸ”’ Security: Solves problems
ğŸ’° Cost: High computational cost
```

### Best Practice
```
1ï¸âƒ£ Start with Pipeline â†’ Measure vulnerability
2ï¸âƒ£ If vulnerable â†’ Use Training
3ï¸âƒ£ Verify with Pipeline â†’ Confirm improvement
4ï¸âƒ£ Deploy â†’ Robust model ready
```

---

## Quick Decision Guide

```
START
  â”‚
  â”œâ”€ Do you need a robust model for deployment?
  â”‚  â””â”€ YES â†’ Use Adversarial Training
  â”‚  â””â”€ NO â†’ Continue
  â”‚
  â”œâ”€ Do you want to test an existing model?
  â”‚  â””â”€ YES â†’ Use Adversarial Pipeline
  â”‚  â””â”€ NO â†’ Continue
  â”‚
  â”œâ”€ Do you have limited time/resources?
  â”‚  â””â”€ YES â†’ Use Adversarial Pipeline (quick check)
  â”‚  â””â”€ NO â†’ Use both (comprehensive)
  â”‚
  â””â”€ Unsure?
     â””â”€ Recommended: Run Pipeline first, then decide
```

---

## Conclusion

Both tools serve **complementary purposes**:

- **Pipeline**: Quick diagnosis ("How vulnerable am I?")
- **Training**: Long-term solution ("Make me robust!")

**Recommended workflow**: Pipeline â†’ Training â†’ Pipeline
1. Evaluate vulnerability
2. Build robustness
3. Verify improvement

This ensures you build robust models based on measured needs, not assumptions.
